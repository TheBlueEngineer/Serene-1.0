{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character-Level CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsPU-1AWlFaB",
        "colab_type": "text"
      },
      "source": [
        "# **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOsPbEPovBAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce66bc2c-bdee-4af9-89af-a562861ca381"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6EPHWUwj-ua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8faa73fa-3c41-458a-e404-ff4ec74411f1"
      },
      "source": [
        "# ***********************\n",
        "# *****| LIBRARIES |*****\n",
        "# ***********************\n",
        "%tensorflow_version 2.x\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print(\"GPU not found\")\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKNWF36Tj-6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ******************************\n",
        "# *****| GLOBAL VARIABLES |*****\n",
        "# ******************************\n",
        "test_size = 0.2\n",
        "\n",
        "convsize = 256\n",
        "convsize2 = 1024\n",
        "embedding_size = 27\n",
        "input_size = 1000\n",
        "conv_layers = [\n",
        "    [convsize, 7, 3],\n",
        "    [convsize, 7, 3],\n",
        "    [convsize, 3, -1],\n",
        "    [convsize, 3, -1],\n",
        "    [convsize, 3, -1],\n",
        "    [convsize, 3, 3]\n",
        "    ]\n",
        "\n",
        "fully_connected_layers = [convsize2, convsize2]\n",
        "num_of_classes= 2\n",
        "dropout_p = 0.5\n",
        "optimizer= 'adam'\n",
        "batch = 128\n",
        "loss = 'categorical_crossentropy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glBHhlV6lJrV",
        "colab_type": "text"
      },
      "source": [
        "# **Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxfuf6p_j_BP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# *****************\n",
        "# *** GET FILES ***\n",
        "# *****************\n",
        "def getFiles( driverPath, directory, basename, extension):  # Define a function that will return a list of files\n",
        "    pathList = []                                           # Declare an empty array\n",
        "    directory = os.path.join( driverPath, directory)        # \n",
        "    \n",
        "    for root, dirs, files in os.walk( directory):           # Iterate through roots, dirs and files recursively\n",
        "        for file in files:                                  # For every file in files\n",
        "            if os.path.basename(root) == basename:          # If the parent directory of the current file is equal with the parameter\n",
        "                if file.endswith('.%s' % (extension)):      # If the searched file ends in the parameter\n",
        "                    path = os.path.join(root, file)         # Join together the root path and file name\n",
        "                    pathList.append(path)                   # Append the new path to the list\n",
        "    return pathList  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I29vcETFko-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ****************************************\n",
        "# *** GET DATA INTO A PANDAS DATAFRAME ***\n",
        "# ****************************************\n",
        "def getDataFrame( listFiles, maxFiles, minWords, limit):\n",
        "    counter_real, counter_max, limitReached = 0, 0, 0\n",
        "    text_list, label_list = [], []\n",
        "\n",
        "    print(\"Word min set to: %i.\" % ( minWords))\n",
        "    # Iterate through all the files\n",
        "    for file in listFiles:\n",
        "        # Open each file and look into it\n",
        "        with open(file) as f:\n",
        "            if(limitReached):\n",
        "              break\n",
        "            if maxFiles == 0:\n",
        "                break\n",
        "            else:\n",
        "                maxFiles -= 1\n",
        "            objects = json.loads( f.read())['data']                  # Get the data from the JSON file\n",
        "            # Look into each object from the file and test for limiters\n",
        "            for object in objects:\n",
        "              if limit > 0 and counter_real >= (limit * 1000):\n",
        "                limitReached = 1\n",
        "                break\n",
        "              if len( object['text'].split()) >= minWords:\n",
        "                text_list.append(object['text'])\n",
        "                label_list.append(object['label'])\n",
        "                counter_real += 1\n",
        "              counter_max += 1\n",
        "\n",
        "    if(counter_real > 0 and counter_max > 0):\n",
        "      ratio = counter_real / counter_max * 100\n",
        "    else:\n",
        "      ratio = 0\n",
        "    # Print the final result\n",
        "    print(\"Lists created with %i/%i (%.2f%%) data objects.\" % ( counter_real, counter_max, ratio))\n",
        "    print(\"Rest ignored due to minimum words limit of %i or the limit of %i data objects maximum.\" % ( minWords, limit * 1000))\n",
        "    # Return the final Pandas DataFrame\n",
        "    return text_list, label_list, counter_real"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waV4k2LslPVB",
        "colab_type": "text"
      },
      "source": [
        "# **Gather the path to files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsK8nTuekpGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "93423507-2669-4c72-84e9-b9d76f2ec1eb"
      },
      "source": [
        "# ***********************************\n",
        "# *** GET THE PATHS FOR THE FILES ***\n",
        "# ***********************************\n",
        "\n",
        "# Path to the content of the Google Drive \n",
        "driverPath = \"/content/drive/My Drive\"\n",
        "\n",
        "# Sub-directories in the driver\n",
        "paths = [\"processed/depression/submission\",\n",
        "         \"processed/depression/comment\", \n",
        "         \"processed/AskReddit/submission\", \n",
        "         \"processed/AskReddit/comment\"]\n",
        "\n",
        "files = [None] * len(paths)\n",
        "for i in range(len(paths)):\n",
        "  files[i] = getFiles( driverPath, paths[i], \"text\", \"json\")\n",
        "  print(\"Gathered %i files from %s.\" % ( len(files[i]), paths[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gathered 750 files from processed/depression/submission.\n",
            "Gathered 2892 files from processed/depression/comment.\n",
            "Gathered 1311 files from processed/AskReddit/submission.\n",
            "Gathered 5510 files from processed/AskReddit/comment.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4CG6WZdlTPq",
        "colab_type": "text"
      },
      "source": [
        "# **Gather the data from files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AgTqBpmktmF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "ec35627e-59b7-4065-8e06-c40a000c94c0"
      },
      "source": [
        "# ************************************\n",
        "# *** GATHER THE DATA AND SPLIT IT ***\n",
        "# ************************************\n",
        "# Local variables\n",
        "rand_state_splitter = 1000\n",
        "test_size = 0.2\n",
        "\n",
        "min_files = [ 750, 0, 1300, 0] \n",
        "max_words = [ 50, 0, 50, 0]\n",
        "limit_packets = [300, 0, 300, 0]\n",
        "message = [\"Depression submissions\", \"Depression comments\", \"AskReddit submissions\", \"AskReddit comments\"]\n",
        "text, label = [], []\n",
        "\n",
        "# Get the pandas data frames for each category\n",
        "print(\"Build the Pandas DataFrames for each category.\")\n",
        "for i in range(4):\n",
        "  dummy_text, dummy_label, counter = getDataFrame( files[i], min_files[i], max_words[i], limit_packets[i])\n",
        "  if counter > 0:\n",
        "    text += dummy_text\n",
        "    label += dummy_label\n",
        "    dummy_text, dummy_label = None, None\n",
        "    print(\"Added %i samples to data list: %s.\\n\" % ( counter ,message[i]) )\n",
        "\n",
        "# Splitting the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(text, \n",
        "                                                    label, \n",
        "                                                    test_size = test_size, \n",
        "                                                    shuffle = True, \n",
        "                                                    random_state = rand_state_splitter)\n",
        "print(\"Training data: %i samples.\" % ( len(y_train)) )\n",
        "print(\"Testing data: %i samples.\" % ( len(y_test)) )\n",
        "\n",
        "# Clear data no longer needed\n",
        "del rand_state_splitter, min_files, max_words, message, dummy_label, dummy_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build the Pandas DataFrames for each category.\n",
            "Word min set to: 50.\n",
            "Lists created with 300000/349305 (85.88%) data objects.\n",
            "Rest ignored due to minimum words limit of 50 or the limit of 300000 data objects maximum.\n",
            "Added 300000 samples to data list: Depression submissions.\n",
            "\n",
            "Word min set to: 0.\n",
            "Lists created with 0/0 (0.00%) data objects.\n",
            "Rest ignored due to minimum words limit of 0 or the limit of 0 data objects maximum.\n",
            "Word min set to: 50.\n",
            "Lists created with 300000/554781 (54.08%) data objects.\n",
            "Rest ignored due to minimum words limit of 50 or the limit of 300000 data objects maximum.\n",
            "Added 300000 samples to data list: AskReddit submissions.\n",
            "\n",
            "Word min set to: 0.\n",
            "Lists created with 0/0 (0.00%) data objects.\n",
            "Rest ignored due to minimum words limit of 0 or the limit of 0 data objects maximum.\n",
            "Training data: 480000 samples.\n",
            "Testing data: 120000 samples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdWMndkClitw",
        "colab_type": "text"
      },
      "source": [
        "# **Process the data at a character-level**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUBX-d44ktsR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "f3689693-d602-4e0b-db53-7b5e584cea52"
      },
      "source": [
        "# *******************************\n",
        "# *** CONVERT STRING TO INDEX ***\n",
        "# *******************************\n",
        "print(\"Convert the strings to indexes.\")\n",
        "tk = Tokenizer(num_words = None, char_level = True, oov_token='UNK')\n",
        "tk.fit_on_texts(x_train)\n",
        "print(\"Original:\", x_train[0])\n",
        "# *********************************\n",
        "# *** CONSTRUCT A NEW VOCABULARY***\n",
        "# *********************************\n",
        "print(\"Construct a new vocabulary\")\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "print(\"dictionary\")\n",
        "tk.word_index = char_dict.copy()                                # Use char_dict to replace the tk.word_index\n",
        "print(tk.word_index)\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1       # Add 'UNK' to the vocabulary\n",
        "print(tk.word_index)\n",
        "# *************************\n",
        "# *** TEXT TO SEQUENCES ***\n",
        "# *************************\n",
        "print(\"Text to sequence.\")\n",
        "x_train = tk.texts_to_sequences(x_train)\n",
        "x_test = tk.texts_to_sequences(x_test)\n",
        "print(\"After sequences:\", x_train[0])\n",
        "# ***************\n",
        "# *** PADDING ***\n",
        "# ***************\n",
        "print(\"Padding the sequences.\")\n",
        "x_train = pad_sequences( x_train, maxlen = input_size, padding = 'post')\n",
        "x_test = pad_sequences( x_test, maxlen= input_size , padding = 'post')\n",
        "\n",
        "# ************************\n",
        "# *** CONVERT TO NUMPY ***\n",
        "# ************************\n",
        "print(\"Convert to Numpy arrays\")\n",
        "x_train = np.array( x_train, dtype = 'float32')\n",
        "x_test = np.array(x_test, dtype = 'float32')\n",
        "\n",
        "# **************************************\n",
        "# *** GET CLASSES FOR CLASSIFICATION ***\n",
        "# **************************************\n",
        "y_test_copy = y_test\n",
        "y_train_list = [x-1 for x in y_train]\n",
        "y_test_list = [x-1 for x in y_test]\n",
        "\n",
        "y_train = to_categorical( y_train_list, num_of_classes)\n",
        "y_test = to_categorical( y_test_list, num_of_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Convert the strings to indexes.\n",
            "Original: i did not think i had have to post in this subreddit i just feel empty and completely alone i am hanging out with friends but nothing makes me feel happy as i used to be i know people generally have it worse i just want someone to talk to and just be silly with \n",
            "Construct a new vocabulary\n",
            "dictionary\n",
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'UNK': 27}\n",
            "Text to sequence.\n",
            "After sequences: [9, 27, 4, 9, 4, 27, 14, 15, 20, 27, 20, 8, 9, 14, 11, 27, 9, 27, 8, 1, 4, 27, 8, 1, 22, 5, 27, 20, 15, 27, 16, 15, 19, 20, 27, 9, 14, 27, 20, 8, 9, 19, 27, 19, 21, 2, 18, 5, 4, 4, 9, 20, 27, 9, 27, 10, 21, 19, 20, 27, 6, 5, 5, 12, 27, 5, 13, 16, 20, 25, 27, 1, 14, 4, 27, 3, 15, 13, 16, 12, 5, 20, 5, 12, 25, 27, 1, 12, 15, 14, 5, 27, 9, 27, 1, 13, 27, 8, 1, 14, 7, 9, 14, 7, 27, 15, 21, 20, 27, 23, 9, 20, 8, 27, 6, 18, 9, 5, 14, 4, 19, 27, 2, 21, 20, 27, 14, 15, 20, 8, 9, 14, 7, 27, 13, 1, 11, 5, 19, 27, 13, 5, 27, 6, 5, 5, 12, 27, 8, 1, 16, 16, 25, 27, 1, 19, 27, 9, 27, 21, 19, 5, 4, 27, 20, 15, 27, 2, 5, 27, 9, 27, 11, 14, 15, 23, 27, 16, 5, 15, 16, 12, 5, 27, 7, 5, 14, 5, 18, 1, 12, 12, 25, 27, 8, 1, 22, 5, 27, 9, 20, 27, 23, 15, 18, 19, 5, 27, 9, 27, 10, 21, 19, 20, 27, 23, 1, 14, 20, 27, 19, 15, 13, 5, 15, 14, 5, 27, 20, 15, 27, 20, 1, 12, 11, 27, 20, 15, 27, 1, 14, 4, 27, 10, 21, 19, 20, 27, 2, 5, 27, 19, 9, 12, 12, 25, 27, 23, 9, 20, 8, 27]\n",
            "Padding the sequences.\n",
            "Convert to Numpy arrays\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goUDgASilmx6",
        "colab_type": "text"
      },
      "source": [
        "# **Load embedding words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_I3SDSqkpJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "outputId": "cf845985-d0ae-4710-d842-6f1c033efa3f"
      },
      "source": [
        "# ***********************\n",
        "# *** LOAD EMBEDDINGS ***\n",
        "# ***********************\n",
        "embedding_weights = []\n",
        "vocab_size = len(tk.word_index)\n",
        "embedding_weights.append(np.zeros(vocab_size))\n",
        "\n",
        "for char, i in tk.word_index.items():\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i-1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "\n",
        "print(\"Vocabulary size: \",vocab_size)\n",
        "print(\"Embedding weights: \", embedding_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  27\n",
            "Embedding weights:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmWTcu3Zlszw",
        "colab_type": "text"
      },
      "source": [
        "# **Build the CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffjuFA8lls9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KerasModel():\n",
        "    # ***************************************\n",
        "    # *****| BUILD THE NEURAL NETWORK |******\n",
        "    # ***************************************\n",
        "    embedding_layer = Embedding(vocab_size+1,\n",
        "                                embedding_size,\n",
        "                                input_length = input_size,\n",
        "                                weights = [embedding_weights])\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
        "\n",
        "    # Embedding layer\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    # Convolution\n",
        "    for filter_num, filter_size, pooling_size in conv_layers:\n",
        "        x = Conv1D(filter_num, filter_size)(x)\n",
        "        x = Activation('relu')(x)\n",
        "        if pooling_size != -1:\n",
        "            x = MaxPooling1D( pool_size = pooling_size)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Fully Connected layers\n",
        "    for dense_size in fully_connected_layers:\n",
        "            x = Dense( dense_size, activation='relu')(x)\n",
        "            x = Dropout( dropout_p)(x)\n",
        "\n",
        "    # Output Layer\n",
        "    predictions = Dense(num_of_classes, activation = 'softmax')(x)\n",
        "\n",
        "    # BUILD MODEL\n",
        "    model = Model( inputs = inputs, outputs = predictions)\n",
        "    model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijiRTE8Pl40f",
        "colab_type": "text"
      },
      "source": [
        "# **Train the CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsA_pmvHl47a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64383d9a-568b-45c7-9dc2-5b7a5845c2a2"
      },
      "source": [
        "#with tf.device(\"/gpu:0\"):\n",
        "#    history = model.fit(x_train, y_train,\n",
        "#            validation_data = ( x_test, y_test),\n",
        "#            epochs = 10,\n",
        "#            batch_size = batch,\n",
        "#            verbose = True)\n",
        "    \n",
        "with tf.device(\"/gpu:0\"):\n",
        "    grid = KerasClassifier(build_fn = KerasModel, epochs = 15, verbose= True)\n",
        "    param_grid = dict(\n",
        "                    epochs = [15]\n",
        "                  )\n",
        "    #grid = GridSearchCV(estimator = model, \n",
        "    #                    param_grid = param_grid,\n",
        "    #                    cv = 5, \n",
        "    #                    verbose = 10,  \n",
        "    #                    return_train_score = True)\n",
        "    \n",
        "    grid_result = grid.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 1000, 27)          756       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 994, 256)          48640     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 994, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 331, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 325, 256)          459008    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 325, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 108, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 106, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 106, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 104, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 104, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 102, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 102, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 100, 256)          196864    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 33, 256)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8448)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              8651776   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 10,999,286\n",
            "Trainable params: 10,999,286\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "480000/480000 [==============================] - 309s 644us/step - loss: 0.2042 - accuracy: 0.9238\n",
            "Epoch 2/15\n",
            "480000/480000 [==============================] - 302s 628us/step - loss: 0.1674 - accuracy: 0.9400\n",
            "Epoch 3/15\n",
            "480000/480000 [==============================] - 302s 630us/step - loss: 0.1594 - accuracy: 0.9430\n",
            "Epoch 4/15\n",
            "480000/480000 [==============================] - 302s 630us/step - loss: 0.1548 - accuracy: 0.9453\n",
            "Epoch 5/15\n",
            "480000/480000 [==============================] - 303s 631us/step - loss: 0.1504 - accuracy: 0.9467\n",
            "Epoch 6/15\n",
            "429984/480000 [=========================>....] - ETA: 31s - loss: 0.1469 - accuracy: 0.9481"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5904af6385fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#                    return_train_score = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjSrIzl6mC2A",
        "colab_type": "text"
      },
      "source": [
        "# **Test the CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpMILRyqmC79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loss, accuracy = model.evaluate( x_train, y_train, verbose = True)\n",
        "#print(\"Training Accuracy: {:.4f}\".format( accuracy))\n",
        "#loss, accuracy = model.evaluate( x_test, y_test, verbose = True)\n",
        "#print(\"Testing Accuracy:  {:.4f}\".format( accuracy))\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_predict = grid.predict( x_test)\n",
        "# Build the confusion matrix \n",
        "y_tested = y_test\n",
        "print( type(y_test))\n",
        "print(y_tested)\n",
        "y_tested = np.argmax( y_tested, axis = 1)\n",
        "print(y_tested)\n",
        "confMatrix = confusion_matrix(y_tested, y_predict)   \n",
        "tn, fp, fn, tp = confMatrix.ravel()  \n",
        "# Build a classification report                       \n",
        "classification_reports = classification_report( y_tested, y_predict, target_names = ['Non-depressed', 'Depressed'], digits=3)\n",
        "print(confMatrix)\n",
        "print(classification_reports)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}